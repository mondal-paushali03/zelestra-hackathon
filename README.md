# âš¡ Zelestra 2025 Hackathon â€“ Solar Panel Efficiency Prediction  

## ğŸ“Œ Overview  
This project was developed for the **Zelestra 2025 Hackathon**, where the challenge was to **predict the efficiency of solar panels** using environmental, operational, and installation-related data.  

We built a **hybrid ensemble machine learning pipeline** that combines **LightGBM**, **Histogram Gradient Boosting**, and **Ridge Regression** inside a **stacking framework**, with advanced feature transformations for robust and accurate efficiency predictions.  

---

## ğŸ—‚ï¸ Dataset  

The dataset contained two files:  

- **train.csv** â€“ Training data with features and target `efficiency`.  
- **test.csv** â€“ Test data with the same features (without efficiency).  

### Key Features:  
- Environmental: `temperature`, `irradiance`, `humidity`, `wind_speed`, `pressure`, `cloud_coverage`  
- Operational: `voltage`, `current`, `module_temperature`, `soiling_ratio`, `maintenance_count`  
- Installation: `panel_age`, `string_id`, `error_code`, `installation_type`  
- **Target**: `efficiency`  

---

## âš™ï¸ Approach  

### ğŸ”¹ 1. Data Preprocessing  
- Converted `wind_speed`, `pressure`, and `humidity` to numeric.  
- Label-encoded categorical variables: `string_id`, `error_code`, `installation_type`.  
- Handled missing values via **KNN imputation**.  

### ğŸ”¹ 2. Feature Engineering  
- Generated **correlation heatmaps** to identify the most important features.  
- Selected features strongly correlated with `efficiency`.  
- Designed a **FeatureBlender transformer**:  
  - StandardScaler  
  - QuantileTransformer (normal distribution)  
  - Original + transformed features concatenated  

### ğŸ”¹ 3. Model Building  
- **Hybrid Stacking Regressor**:  
  - Base learners:  
    - `LightGBM Regressor`  
    - `Histogram Gradient Boosting Regressor`  
  - Final learner: `RidgeCV`  
- Implemented using **Scikit-learn Pipelines** for clean workflow.  

### ğŸ”¹ 4. Evaluation  
- Metrics:  
  - **Mean Squared Error (MSE)**  
  - **RÂ² Score**  
  - **Mean Absolute Percentage Error (MAPE)**  
- Custom hackathon score function was also computed.  

---

## ğŸš€ Results  
- The hybrid stacking model achieved **robust accuracy in efficiency prediction**.  
- Strong generalization on test data.  
- Final predictions.
---

## ğŸ“‚ Files in Repository  
- `train.csv` â€“ Training dataset  
- `test.csv` â€“ Test dataset  
- `zelestra.ipynb` â€“ Main notebook with preprocessing, modeling, and prediction code  
---

## â–¶ï¸ How to Run  

1. Clone the repository:  
   ```bash
   git clone https://github.com/mondal-paushali03/zelestra-2025-hackathon.git
   cd zelestra-2025-hackathon ```
2. Upload datasets (train.csv, test.csv) in Colab or local environment.
3. Run the notebook:
   ```bash
   jupyter notebook hackathon_solution.ipynb```

---
## ğŸ“Š Tech Stack  

- **Python**  
- **Pandas, NumPy, Matplotlib, Seaborn** â€“ EDA & Visualization  
- **Scikit-learn** â€“ Preprocessing, Pipelines, Stacking  
- **LightGBM** â€“ Boosting Model  
- **Google Colab** â€“ Execution Environment
---

## Contact
For questions or suggestions, feel free to reach out at mondal.paushali384@gmail.com.

## License
This project is licensed under the MIT License.


